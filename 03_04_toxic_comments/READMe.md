# Обнаружение токсичных комментариев

## Задача

Построить модель, которая будет выявлять токсичные комментарии. Метрика качества модели F1 должна получиться не меньше 0.75.

## Используемые библиотеки

*lightgbm* *matplotlib* *nltk* *numpy* *pandas* *pytorch* *seaborn* *sklearn* *tqdm* *transformers*

## Данные

В наличии набор размеченных комментариев. Столбец text содержит текст комментария, а toxic — целевой признак (1 - комментарий токсичный, 0 - нет).

## Результат

Были испробованы различные техники NLP, включая tfidf и bert. Лучшая метрика F1 = 0.78 была достигнута на линейной модели SVC.