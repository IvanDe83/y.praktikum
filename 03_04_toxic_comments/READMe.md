# Обнаружение токсичных комментариев


## Данные

В наличии набор размеченных комментариев. Столбец text содержит текст комментария, а toxic — целевой признак (1 - комментарий токсичный, 0 - нет).

## Задача

Построить модель, которая будет выявлять токсичные комментарии. Метрика качества модели F1 должна получиться не меньше 0.75.

## Используемые библиотеки

*lightgbm* *matplotlib* *nltk* *numpy* *pandas* *pytorch* *seaborn* *sklearn* *tqdm* *transformers*